---
title: Test Implementation Plan for Reader Observability
by: khive-implementer
created: 2025-05-22
updated: 2025-05-22
version: 1.0
doc_type: TI
output_subdir: ti
description: Test implementation plan for the observability features (Prometheus metrics, performance thresholds) of the Khive Reader Microservice.
date: 2025-05-22
issue_ref: 29
---

# Guidance

**Purpose** Document the planned and actual test implementation. Clarify unit,
integration, performance, mocking details, and test data.

**When to Use**

- Before/during writing tests, especially if itâ€™s a large feature or
  microservice.
- As a blueprint to ensure coverage is complete.

**Best Practices**

- Keep tests short and focused.
- Use mocking for external calls.
- Outline coverage goals.

---

# Test Implementation Plan: Reader Microservice Observability (Issue #29)

## 1. Overview

### 1.1 Component Under Test

The new monitoring components for the Khive Reader Microservice, including:

- [`src/khive/reader/monitoring/prometheus.py`](src/khive/reader/monitoring/prometheus.py:0)
  (metrics definitions, `monitor_async` decorator, background updates)
- [`src/khive/reader/monitoring/thresholds.py`](src/khive/reader/monitoring/thresholds.py:0)
  (`PerformanceThreshold`, `PerformanceMonitor`)
- [`src/khive/reader/monitoring/middleware.py`](src/khive/reader/monitoring/middleware.py:0)
  (`PrometheusMiddleware`)
- [`src/khive/reader/monitoring/__init__.py`](src/khive/reader/monitoring/__init__.py:0)
  (`start_monitoring` function)
- Integration of `@monitor_async` into existing services
  (`DocumentSearchService`, `DocumentProcessingService`,
  `DocumentIngestionService`).
- CLI performance check command in
  [`src/khive/cli/khive_reader.py`](src/khive/cli/khive_reader.py:0).

### 1.2 Test Approach

Primarily unit tests for individual functions and classes. Integration tests
will focus on the correct application of decorators and initialization logic.
Aim for >=80% test coverage for all new code.

### 1.3 Key Testing Goals

- Verify that Prometheus metrics are correctly defined and updated by the
  `monitor_async` decorator and background tasks.
- Ensure `PerformanceThreshold` logic correctly identifies when thresholds are
  exceeded or recovered.
- Confirm `PerformanceMonitor` can check all defined thresholds and its
  placeholder check functions behave as expected.
- Validate the `PrometheusMiddleware` correctly instruments HTTP requests (basic
  check).
- Ensure the `start_monitoring` function correctly initializes components.
- Verify the CLI `performance` command correctly retrieves and displays
  threshold status.
- Confirm error handling within monitoring components.

## 2. Test Environment

### 2.1 Test Framework

```python
# Python
pytest
pytest-asyncio  # For testing async code
pytest-mock     # For mocking dependencies
pytest-cov      # For test coverage reporting
```

### 2.2 Mock Framework

```python
# For Python
unittest.mock   # Standard library mocking
pytest-mock     # pytest fixture for unittest.mock
```

### 2.3 Test Database

For unit tests, database interactions (e.g., in `update_metrics_periodically`,
`_check_vector_count`) will be mocked. No actual database will be used for these
tests.

## 3. Unit Tests

Details for unit tests are outlined in the Implementation Plan
([`IP-29.md`](.khive/reports/ip/IP-29.md:0)), section "3.1 Unit Tests". This
includes:

- Test Group: Prometheus Metrics (`prometheus.py`) (IDs UT-P1 to UT-P5)
- Test Group: Performance Thresholds (`thresholds.py`) (IDs UT-T1 to UT-T6)
- Test Group: CLI Performance Check (`khive_reader.py`) (IDs UT-C1 to UT-C2)

### Example Test Case Structure (Prometheus `monitor_async`)

#### Test Case: `monitor_async` increments counter on success (UT-P1)

**Purpose:** Verify the `monitor_async` decorator correctly increments the
specified Prometheus Counter when the decorated async function executes
successfully. **Setup:**

```python
import pytest
from unittest.mock import AsyncMock, MagicMock
from khive.reader.monitoring.prometheus import monitor_async, SEARCH_REQUESTS

@pytest.fixture
def mock_search_requests_counter():
    original_inc = SEARCH_REQUESTS.labels(status="success").inc
    SEARCH_REQUESTS.labels(status="success").inc = MagicMock()
    yield SEARCH_REQUESTS.labels(status="success").inc
    SEARCH_REQUESTS.labels(status="success").inc = original_inc # Restore

@pytest.mark.asyncio
async def test_monitor_async_search_success(mock_search_requests_counter):
    # Arrange
    @monitor_async("search")
    async def dummy_search_function():
        return "search successful"

    # Act
    result = await dummy_search_function()

    # Assert
    assert result == "search successful"
    mock_search_requests_counter.assert_called_once()
```

### Example Test Case Structure (PerformanceThreshold)

#### Test Case: `PerformanceThreshold.check` when value exceeds threshold (UT-T2)

**Purpose:** Verify `PerformanceThreshold.check` correctly identifies an
exceeded threshold and calls the alert handler. **Setup:**

```python
import pytest
import time
from unittest.mock import AsyncMock
from khive.reader.monitoring.thresholds import PerformanceThreshold

@pytest.mark.asyncio
async def test_performance_threshold_exceeded():
    # Arrange
    mock_check_func = AsyncMock(return_value=150.0)
    mock_alert_handler = AsyncMock()
    
    threshold = PerformanceThreshold(
        name="test_metric",
        description="A test metric",
        threshold_value=100.0,
        check_function=mock_check_func,
        alert_handler=mock_alert_handler
    )

    # Act
    exceeded = await threshold.check()

    # Assert
    assert exceeded is True
    assert threshold.exceeded is True
    assert threshold.last_value == 150.0
    mock_check_func.assert_called_once()
    mock_alert_handler.assert_called_once_with(
        "test_metric", "A test metric", 150.0, 100.0
    )
```

## 4. Integration Tests

Details for integration tests are outlined in
[`IP-29.md`](.khive/reports/ip/IP-29.md:0), section "3.2 Integration Tests".

- IT-1: Test monitored service method updates Prometheus metrics.
- IT-2: Test `start_monitoring` initializes Prometheus server and background
  tasks.

### Example Integration Test Structure (Service Decorator)

#### Test Case: Monitored service method updates Prometheus metrics (IT-1)

**Purpose:** Verify that when a service method decorated with `@monitor_async`
is called, the corresponding Prometheus metric is updated. **Setup:**

```python
import pytest
from unittest.mock import patch, MagicMock
from khive.reader.monitoring.prometheus import SEARCH_LATENCY # Assuming this is the relevant metric
# from khive.services.reader.search_service import DocumentSearchService # Actual service

# Minimal mock service for demonstration
class MockSearchService:
    @monitor_async("search") # Assuming monitor_async is imported
    async def search(self, query: str):
        return f"results for {query}"

@pytest.mark.asyncio
async def test_search_service_updates_search_latency():
    # Arrange
    # Patch the actual Prometheus Histogram object's observe method
    with patch.object(SEARCH_LATENCY, 'observe') as mock_observe:
        service = MockSearchService() # Or the actual DocumentSearchService if easily instantiable

        # Act
        await service.search(query="test query")

        # Assert
        mock_observe.assert_called_once() 
        # Further assertions can be made on the value observed if needed, by checking mock_observe.call_args
```

## 5. API Tests

Since the web API itself is future work according to Issue #29, full API tests
are not in scope for this implementation. Basic unit tests for the
`PrometheusMiddleware` will be created to check if it attempts to update
metrics.

## 6. Error Handling Tests

Error handling will be tested as part of the unit tests for each component:

- `monitor_async` handling exceptions in decorated functions (UT-P3).
- `update_metrics_periodically` handling DB errors (UT-P5).
- `PerformanceThreshold.check` handling exceptions in its `check_function`
  (UT-T4).
- CLI `check_performance` handling errors from `PerformanceMonitor` (UT-C2).

## 7. Performance Tests

Formal performance tests (benchmarking/load testing) are out of scope for this
issue. The implemented "Performance Thresholds" are for monitoring, not for
performance testing itself.

## 8. Mock Implementation Details

Mocks will be created using `unittest.mock.MagicMock` and
`unittest.mock.AsyncMock` as needed. Key mocks include:

- Prometheus metric objects (`Counter`, `Histogram`, `Gauge`) to verify `inc()`,
  `observe()`, `set()`.
- DB session factory (`get_db_session`) to return a mock session with a mock
  `execute()` method.
- `task_queue` to provide `pending_tasks`.
- `asyncio.sleep` to control timing in tests of periodic tasks.
- `logging` to capture and verify log messages.

Example Mock for DB Session:

```python
from unittest.mock import AsyncMock, MagicMock

def create_mock_db_session_factory(scalar_results=None):
    if scalar_results is None:
        scalar_results = [0] # Default scalar result

    mock_session = MagicMock()
    
    # Make execute return an object that has a scalar method
    # Cycle through scalar_results if multiple calls to execute are expected
    result_iter = iter(scalar_results)
    
    async def mock_execute(*args, **kwargs):
        mock_result = MagicMock()
        mock_result.scalar = MagicMock(return_value=next(result_iter, 0))
        return mock_result
        
    mock_session.execute = AsyncMock(side_effect=mock_execute)
    
    mock_session_factory = MagicMock()
    mock_session_factory.return_value.__aenter__.return_value = mock_session # For async with
    return mock_session_factory
```

## 9. Test Data

Specific test data will be simple values (e.g., query strings, counts) passed to
mocked functions or returned by them. No complex test data fixtures are
anticipated for the monitoring logic itself.

## 10. Helper Functions

Test helper functions might be created for:

- Setting up specific mock configurations for Prometheus metrics.
- Simplifying the instantiation of `PerformanceThreshold` or
  `PerformanceMonitor` with mocked dependencies.

## 11. Test Coverage Targets

- **Line Coverage Target:** >=80% for all new files in
  `src/khive/reader/monitoring/` and new code in
  `src/khive/cli/khive_reader.py`.
- **Branch Coverage Target:** Aim for >=75%.
- **Critical Modules:** `prometheus.py` and `thresholds.py` should have high
  coverage.

## 12. Continuous Integration

Tests will be run as part of the existing CI pipeline (presumably configured
with `pytest --cov`).

## 13. Notes and Caveats

### 13.1 Known Limitations

- Middleware testing will be basic due to the absence of a full FastAPI
  application context in unit tests.
- `PerformanceMonitor`'s `_check_search_latency` and
  `_check_db_connection_utilization` are placeholders and their tests will only
  verify they return the placeholder values.

### 13.2 Future Improvements

- More comprehensive integration tests for middleware once the FastAPI
  application is in place.
- Tests for actual alert dispatching (e.g., Slack, Email) if/when implemented
  beyond logging.
